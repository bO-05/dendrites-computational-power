the year 2022 which has recently passed

can easily be called the year of neural

networks after all we got a language

model which can write you a surprisingly

decent assay an AI art generator and

even a platform which turns any image

into anime all of this was made possible

by the development of artificial neural

networks a type of computing system

modeled as a network of interconnected

nodes which can learn to solve problems

by recognizing patterns hidden in the

training data and if you go online and

Google artificial neural networks you

are likely to see statements such as

they work pretty much like your brain

well you see such claims although

attractive can be a bit misleading

because biological neurons are actually

much more powerful than it was

previously thought

in this video we will see why individual

neurons in your brain function

essentially like full-blown neural

networks themselves equipped with insane

information processing capabilities as

well as some of the physiological

mechanisms that account for this

computational complexity if you're

interested stay tuned

before we begin I'd like to warn you

that this video is not on artificial

neural networks per se we're not going

to talk about back propagation or

gradient descent or any of that stuff in

detail but we are going to talk about

their relationship with biological

neurons as well as some of the

applications to Modern Neuroscience but

more on that later early neural networks

were indeed inspired by the descriptions

of biological neurons which people

thought to be accurate at the time in

fact the birth of machine learning as we

know it can be traced back to 1943 when

Walter Pitts and Warren McCulloch

introduced the notion of a perceptron

despite the fancy name the idea is quite

simple

the perceptron was created to function

like an individual nerve cell which in

this Doctrine works like a simple

summator and comparator this just means

that it receives the an input set of

numbers multiplies them by some

coefficients also called weights sums

everything together and compares the

result with a threshold if the resulting

value exceeds the threshold the

perceptron sends a number one as an

output to its neighbors

you interconnect a bunch of these

perceptrons with each other such that

the output of one serves as an input to

a downstream perceptron include an input

and output layer and boom you got a

neural network and to train the network

means to somehow adjust the values of

these input weights to make it match an

input to a correct output

but at that point the fields of machine

learning and neurobiology pretty much

Leverage

over the years people have invented a

bunch of activation functions organized

neurons in a multitude of different

network architectures came up with

algorithms to change the weights

efficiently and so much more but because

we still refer to the nodes in such

networks as neurons a lot of people

believe that biological neurons in the

brain function exactly like their

perceptron counterparts the main goal of

this video is to exonerate biological

neurons and show that single cells are

much more computationally powerful and

sophisticated than you might think but

to understand the computational

complexity of a neuron it's helpful to

remind ourselves of the basic biology

behind neural computations if you open

any Neuroscience textbook one of the

first things you'll see is the structure

of a typical neuron usually consisting

of the dendrites Soma or cell body and

in Axon

let's put dendrites aside from now as

they will become key players later on

you probably know the key property of

neurons is that they are electrically

excitable cells which means they have

the capacity to generate brief

electrical pulses that are propagated to

other neurons forming a basis of

communication between cells in

biological systems electric charge is

carried by ions such as sodium potassium

chloride and calcium which are floating

both inside and outside the cells in

different proportions

cells are separated from the outside

world by a lipid membrane a barrier

normally impermeable to ions however

neurons possess special proteins forming

channels through which specific ions can

cross the membrane and which can open

and close through a variety of

mechanisms as we'll see further

so by regulating the flow of ions

through the shells cells can control the

balance of electric charges and thus

control the membrane voltage namely when

positive ions flow into the cell they

are said to depolarize the membrane

increasing the voltage making the

potential more positive and vice versa

for negative ions

from the whole zoo of ion channels we'll

be mostly interested in What's called

the voltage-gated channels which can

open and close depending on the value of

membrane potential

in school we usually learn about

voltage-gated channels in the context of

action potential generation so just to

remind ourselves an action potential

which is the unit of communication is

often said to be all or none output of a

neuron in a simplified description it is

generated at the origin of an axon where

there is a large number of special

sodium channels that open when the

membrane voltage exceeds a certain

threshold because sodium is a positively

charged ion when it rushes into the cell

the membrane gets depolarized even

further recruiting more sodium channels

to open and so forth like a positive

feedback loop that wave of sodium

channels opening up is transmitted along

the axon and gets indirectly passed onto

other neurons by the means of synaptic

transmission however sodium channels

don't stay open forever and eventually

they close well another type of channels

opens allowing potassium also a

positively charged ion to leave the cell

bringing the membrane voltage back to

its resting level

ta-da we have just generated an action

potential and sent one bit of

information to Downstream neurons you

might already see why the initial

description of a neuron as a perceptron

seemed reasonable

after all we have just witnessed how a

thresholding function is implemented in

a neuron through voltage-gated channels

so that part is accurate right indeed

the main problem with the perceptron is

mostly with inputs not the output it

it's time for us to get back to the

dendrites we've been putting off and see

how exactly they contribute to the

computations on incoming information now

the weight is often presented and this

is the way I was taught is that dead

rights function like passive receivers

of information

you see this signal is transmitted

between the two cells when the action of

one neuron forms a special connection a

synapse with a dendrite of another

neuron upon the action potential of a

sender neuron or pre-synaptic cell there

is a chain of chemical reactions which

leads to a release of signal molecules

and eventually resulting in the opening

of neurotransmitter gated ion channels

on the membrane of the receiver

postsynaptic neuron this flux of ions

across the membrane of our postsynaptic

cell leads to its depolarization which

is propagated along the dendritic tree

and the laws of such propagation of

electrical signals in passive membrane

is conventionally described by a cable

Theory

in that Paradigm the dendrites are just

that cables whose only job is to

transmit electrical signals to the Suma

for those of you who are more interested

dendrites are actually treated as

imperfect leaky cables since a single

portion of a dendrite is described as a

patch of lipid membrane with capacitive

properties and a resistive component

corresponding to a leak of ions through

passive permanently open ion channels as

a result the depolarization level is

attenuated as it travels along the

dendrite in other words the function of

dendrites is reduced to summing the

incoming signals where the weight of a

synaptic input is determined by the

amount of receptors and how far away the

synapse is from the cell body

together with the threshold happening at

the Soma this gives us the biological

basis for the perceptron model

however as neuroscientists studied the

cells more closely it became clear that

dendrites are anything but passive

cables and the reason for that is that

apart from permanently open passive ion

channels which give them their leaky

properties dendrites are also covered by

a myriad of voltage-gated ion channels

which equip the dendritic tree with

powerful information processing

capabilities even more powerful than

this Soma itself for example dendrites

actually contain voltage-gated sodium

channels similar to the ones found on

the Axon they allow action potentials to

travel in Reverse Direction and

influence the postsynaptic sites

this is called back propagation and it

plays an important role in synaptic

plasticity adjusting the weights of

inputs fast sodium channels also give

dendrites an ability to generate their

own small action potential like

depolarizations which can transiently

amplify synaptic inputs

another important type of channel is

nmda receptor which requires both

sufficient membrane depolarization and

the presence of neurotransmitter to open

thus functioning like a kind of

coincidence detector

nmda channel is non-selective to cations

allowing both calcium and sodium to flow

into the cells which has great

implications for things like synaptic

plasticity

this depolarizing event is often called

an nmda Spike

unlike pure sodium spikelets these are

generated by the influx of calcium as

well and are characterized by a much

longer time scale of hundreds of

milliseconds importantly to our

discussion nmda channels allow the

dendrites to perform non-linear

integration of incoming information

this equips dendrites with vast

computational complexity for example

dendrites can discriminate the order of

incoming Action potentials this means

that if you took a group of synapses on

a dendrite of a cortical pyramidal

neuron then a sequential activation of

them in one direction will produce a

fundamentally different electrical and

chemical response than activation in

Reverse Direction and this is sensitive

not only to order but also to the

velocity of activation

thus single neurons have a mechanism to

process temporal patterns and generate

sequence selective output

nmd Spikes have also been shown to

enhance stimulus selectivity in the

visual cortex of awake animals

thus contributing to behaviorally

relevant neural computations in the

intact brain if you're interested in

learning more about this paper and the

key result I have a dedicated short clip

available for my patreon supporters

if you would like to support the Channel

vote for video topics and enjoy the

bonus content you can find more

information by following the link in the

description but today I'd like to focus

on something different

what if I told you that certain neurons

in the human cortex are capable of

performing a type of computation that

was previously thought to require a

multi-layered neural network to

implement indeed in 2020 a group of

researchers from the laboratory led by

Matthew larcom published a paper titled

dendritic Action potentials and

computation in human layer 2-3 cortical

neurons where they demonstrated a unique

property of human pyramidal neurons

more specifically they performed

simultaneous recordings of electrical

activity at the cell body and the

dendrite and found a new type of

electrical response that was initiated

at the dendrites by a sufficiently

strong excitatory inputs

this waveform was termed as dendritic

calcium action potential as the name

suggests such electrical events are

caused by the influx of calcium ions but

have a shorter time scale compared to

nmd spikes

remarkably these calcium spikes which

have not been described in other

mammalian species are highly selective

to a particular input strength this

means that if you stimulate a neuron

with too weak of a current the membrane

voltage would stay below the threshold

for opening of calcium channels so no

dendritic spikes would be observed

if however you increase the current too

much no spikes would be observed either

you would need to provide the dendrite

with just the right strength of

stimulation to trigger them this fact

may not seem like a big deal at first

but it's really important for the way we

should treat biological neurons

to better understand what it really

means for the neural computations let's

talk about logical operations

as you probably know the device you're

watching this on stores the information

in bits binary units with only two

possible States zero and one oh and by

the way I will use terms 0 and 1 and

false and true interchangeably

throughout the rest of the video

to perform computations on binary data

computers use what are called bitwise

operations which are like addition and

multiplication but in binary world

they are performing by logic gates which

are the building blocks of all the

digital Hardware individual logic gates

perform simple Boolean operations which

you have probably heard about with the

two most popular ones being and and or

operations

for example the anti gate receives two

inputs and outputs one if and only if

both of the inputs are one

while the or gate outputs one when at

least one of the inputs is equal to one

this is also known as inclusive or

because the output is true when both of

the inputs are true as well

so if we view the inputs as Venn

diagrams the intersection is included

there is another useful operation which

is called exclusive or or xor for short

as the name suggests the xorgate outputs

true when exactly one of the inputs is

true but not both

by the way notice that this agrees with

our everyday interpretation of the word

or

when we say things like would you like a

cup of coffee or tea the word or is

usually understood in the exclusive

sense

and in computers soar can be used for

things like comparing two numbers

importantly to our today's discussion

xor is what's called a linearly

non-separable function

and this simply means that there is no

line for two Dimensions playing for

three dimensions or hyperplane for n

Dimensions that would separate different

classes of output

for example let's consider a perceptron

which receives just two inputs similarly

to the logic gate call them X and Y as

we have discussed earlier all perceptron

does is multiply the inputs by their

weights adds everything and compares the

result with a threshold if we denote the

weights As A and B then the perceptron

essentially solves the inequality ax

plus b y is bigger or equal than

threshold but if you consider the

geometric representation of this

inequality it's easy to see that this is

essentially a line separating the two

halves of the X Y plane

similarly if perceptron had three inputs

this equation would correspond to a

plane cut in the 3D space into two

halves and so forth

that's why a single perceptron can

function as a classifier when two output

classes are located on opposite sides of

that line in other words when they are

linearly separable

returning back to our logic gates the

inputs would be limited to 0 and 1. if

we visualize the and gate it's easy to

see that there is a line separating the

true and false outputs so a perceptron

can act as an and gate

and it is possible to turn it into an or

gate just by lowering the threshold the

xor gate however is different notice

that there is no line that would

separate the zero outputs from the ones

which makes it a linearly non-separable

function

this is why to perform the xor operation

a multi-layered network is required and

it was believed to be true for

biological neurons as well that

individual cells can't compute the xor

function until of course that paper came

out

remember that they described dendritic

spikes showed a prominent cell activity

towards the strength of a stimulus so

for example let's say the neuron has two

sets of synapses A and B

when either one of the two sets is

activated the excitation is large enough

to elicit and dendritic Spike which can

propagate to the Soma and Trigger the

action potential however if both sets of

synapses are activated at the same time

the strength of incoming current exceeds

the optimal value for the dendritic

spike generation and no such event is

observed in other words the dendrite

just performed the xor operation on A

and B inputs how awesome is that for

those of you who are curious about the

biophysical mechanisms of such

sensitivity this largely remains

unresolved however through computer

simulations the authors were able to

show that this property of dendritic

action potentials can be explained by a

combination of known voltage-gated

calcium channels and special potassium

channels that are sensitive to both

voltage and calcium concentration

but if biological neurons are quite

complex computational devices on their

own

how does this fit into the picture of

existing neural networks that were

developed based on oversimplified

assumptions should we just throw

everything away and start fresh well I

think quite the contrary there is a

promising Future For the synthesis of

ideas from Modern neuroscience and deep

learning in particular I would like to

conclude this video by discussing a very

elegant paper titled single cortical

neurons as deep artificial neural

networks

the authors asked an interesting

question can a deep neural network

accurately capture the complex

information processing of single neurons

in our brains and if so how should this

equivalent Network look like to find an

answer they first created a detailed

biophysically realistic model of a

single cortical neuron using a

reconstructed morphology and populating

it with all sorts of differential

equations describing the Dynamics of

membrane voltage opening and closing of

ion channels of different types fluxes

of ions across the membrane and so forth

this detailed spatial model was

bombarded with this set of incoming

inputs and the voltage Trace at the Soma

was recorded as the output

the authors then trained a deep

convolutional neural network with

various number of layers

to see if it could learn the complex

input output relationship of the

biophysical model when the network

receives the identical set of synaptic

inputs

as a result they found that it required

between 5 and 8 layers to accurately

predict output spikes at voltage values

of the detailed model

interestingly removing nmda channels

from the model drastically reduced the

complexity of the equivalent Network

which now could predict the output with

just a single hidden layer this

demonstrates the importance of dendritic

non-linearalities and nmda channels in

particular for equipping neurons with

vast computational complexity and

surprisingly the Deep neural network

that was trained purely on randomly

scattered synaptic inputs was able to

generalize and could Faithfully predict

the output even for spatially clustered

and synchronously activated synapses

that it had never encountered before

so in a way it was able to grasp the

underlying biophysics of the neuron

which was never explicitly specified

so what does this all tell us

first of all this study suggests that

single cortical neurons with their

non-linear dendritic integration

properties are indeed sophisticated

computational units on their own and

this computation is comparable with a

multi-layered convolutional network

which if you think about it is a pretty

mind-blowing thing for a single cell to

do it also offers a practical advantage

to module individual neurons more

efficiently because even the Deep neural

network with eight layers

is 2 000 times faster than running the

detailed model which requires solving a

myriad of partial differential equations

are you interested in learning more

about the topics discussed here such as

neural networks and differential

equations but not sure where to start

well in that case you are definitely

going to love our today's sponsor

brilliant.org

it is a revolutionary educational

platform which allows you to advance in

stem fields in a fun and engaging way

the key feature of brilliant is that you

get to learn by doing through exploring

interactive demonstrations and solving

problems along the way to ensure that

you grasp even the most complicated

Concepts brilliant offers thousands of

lessons of different levels of

difficulty from math and computer

science fundamentals to things like

vector calculus and computational

biology and the new ones are being added

every month for example if you enjoyed

this video then you might be interested

in a course on artificial neural

networks it covers a lot of Concepts

that you have just watched such as the

perception models linear separability

and convolutional networks in more depth

while also diving into more complex

topics like the advanced Network

architectures don't hesitate to try

brilliant and begin growing your

knowledge base in bite-sized chunks just

by indicating 15 minutes a day follow

the link down in the description to get

started for free and the first 200

people to use it will also get 20 off

Brilliance premium annual membership

alright let's recap in this video we

have seen how the presence of

voltage-gated ion channels turns

dendrites from being merely passive

conductors of electricity to active

computational units moreover individual

dendritic branches have the capacity to

perform exclusive or operation on their

inputs a type of computation that was

previously thought to require

multi-layered networks and finally we

have seen how the complex input output

transformations of information inside

biological neurons require a

computational power of a whole

convolutional deep neural network

so hopefully I was able to convince you

that even at level of single cells the

brain is incredibly complex and

fascinating and that next time you hear

statements that individual neurons

essentially function like linear

summaters you will take those claims

with a grain of salt if you liked the

video share it with your friends

subscribe to the channel if you haven't

already and press the like button

stay tuned for more interesting topics

coming up goodbye and thank you for the

interest in the brain

[Music]

okay

[Music]

